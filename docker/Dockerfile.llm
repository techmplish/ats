# Placeholder for Local LLaMA
# In future, use something like:
# FROM ghcr.io/ggerganov/llama.cpp:server
# CMD ["-m", "/models/llama-2-7b.Q4_K_M.gguf", "--host", "0.0.0.0", "--port", "8080"]
FROM alpine:latest
CMD ["echo", "Local LLaMA placeholder"]
